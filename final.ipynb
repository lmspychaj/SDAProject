{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.fft import ifft\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import ks_2samp\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import uniform, cumfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from new csv file\n",
    "ds_new = pd.read_csv('MB_meteorite_data.csv', delimiter='|')\n",
    "print(ds_new.head())\n",
    "\n",
    "# Drop nan values in Long and Lat\n",
    "ds_new = ds_new.dropna(subset=['Lat', 'Long'])\n",
    "\n",
    "# Split dataset into fallen and found meteorites\n",
    "fallen = ds_new[ds_new['Fall'] == 'Fell']\n",
    "found = ds_new[ds_new['Fall'] == 'Found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from old csv file\n",
    "ds_old = pd.read_csv('Meteorite_Landings.csv')\n",
    "print(ds_old.head())\n",
    "\n",
    "# Drop nan values in Long and Lat\n",
    "ds_old = ds_old.dropna(subset=['reclat', 'reclong'])\n",
    "\n",
    "# Split dataset into fallen and found meteorites\n",
    "fallen_old = ds_old[ds_old['fall'] == 'Fell']\n",
    "found_old = ds_old[ds_old['fall'] == 'Found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare map\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "\n",
    "# Plot meteorites on map\n",
    "#map.plot(dataset['reclong'], dataset['reclat'], 'ro', markersize=3)\n",
    "ax.plot(found['Long'], found['Lat'], 'ro', markersize=3, label='Found')\n",
    "ax.plot(fallen_old['reclong'], fallen_old['reclat'], 'bo', markersize=3, label='Fallen')\n",
    "\n",
    "# Show map\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract years\n",
    "years = fallen_old['year']\n",
    "\n",
    "# Filter years with very little data\n",
    "years = years[years > 1800]\n",
    "yearrange = int(years.max() - 1800) # change 1800 to years.min when using full dataset\n",
    "\n",
    "# Create histogram\n",
    "histogram = plt.hist(years, bins=yearrange)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract counts from histogram\n",
    "counts = histogram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sample points\n",
    "N = yearrange\n",
    "# Sample spacing\n",
    "T = 1.0  # one year\n",
    "\n",
    "# Compute the Fast Fourier Transform (FFT)\n",
    "# The FFT converts the data from the time \n",
    "# domain to the frequency domain, decomposing it into sinusoidal components at different frequencies.\n",
    "yf = fft(counts)\n",
    "xf = fftfreq(N, T)[:N//2]\n",
    "\n",
    "# Compute the Power Spectrum\n",
    "power_spectrum = 2.0/N * np.abs(yf[:N//2])\n",
    "\n",
    "# Plotting the Power Spectrum\n",
    "plt.plot(xf, power_spectrum)\n",
    "plt.title(\"Power Spectrum of Meteor Landings\")\n",
    "plt.xlabel(\"Frequency (1/year)\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Peaks\n",
    "# use the find_peaks function from scipy.signal\n",
    "# Find peaks\n",
    "peaks, _ = find_peaks(power_spectrum, height=0.1)\n",
    "\n",
    "# Plotting the Power Spectrum with Peaks\n",
    "plt.plot(xf, power_spectrum)\n",
    "plt.plot(xf[peaks], power_spectrum[peaks], \"x\")\n",
    "plt.title(\"Power Spectrum of Meteor Landings\")\n",
    "plt.xlabel(\"Frequency (1/year)\")\n",
    "plt.ylabel(\"Power\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Fourier Transform\n",
    "time_domain_reconstructed = ifft(yf)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "\n",
    "# Plot reconstructed signal\n",
    "axs[0].plot(np.arange(1800, 1800 + len(time_domain_reconstructed.real)), time_domain_reconstructed.real, label=\"Reconstructed\")\n",
    "axs[0].set_ylabel(\"Reconstructed Count\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot original signal\n",
    "axs[1].plot(np.arange(1800, 1800 + N), counts, label=\"Original\", alpha=1)\n",
    "axs[1].set_xlabel(\"Year\")\n",
    "axs[1].set_ylabel(\"Original Count\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.suptitle(\"Original vs Reconstructed Signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for significance\n",
    "num_permutations = 1000\n",
    "num_samples = len(counts)\n",
    "\n",
    "# Store Fourier powers for each permutation\n",
    "permuted_fourier_powers = np.zeros((num_permutations, num_samples // 2))\n",
    "\n",
    "# Perform the permutations and Fourier analysis\n",
    "for i in range(num_permutations):\n",
    "    shuffled_counts = np.random.permutation(counts)\n",
    "    fft_result_shuffle = fft(shuffled_counts)\n",
    "    power = 2.0/N * np.abs(fft_result_shuffle[:N//2])\n",
    "    permuted_fourier_powers[i, :] = power\n",
    "\n",
    "# Calculate mean and standard deviation for each frequency component's power\n",
    "mean_powers = np.mean(permuted_fourier_powers, axis=0)\n",
    "std_powers = np.std(permuted_fourier_powers, axis=0)\n",
    "\n",
    "# Calculate the 95% confidence intervals for the power\n",
    "confidence_interval_95_upper = mean_powers + 1.96 * std_powers\n",
    "confidence_interval_95_lower = mean_powers - 1.96 * std_powers\n",
    "\n",
    "# Plot the original power spectrum with the confidence intervals\n",
    "plt.fill_between(xf, confidence_interval_95_lower, confidence_interval_95_upper, color='gray', alpha=0.5)\n",
    "plt.plot(xf, power_spectrum, label='Original Power Spectrum')\n",
    "plt.legend()\n",
    "plt.xlabel('Frequency (1/year)')\n",
    "plt.ylabel('Power')\n",
    "plt.title('95% Confidence Intervals of Shuffled Data Fourier Power')\n",
    "plt.show()\n",
    "\n",
    "# Identify significant peaks\n",
    "significant_peaks = power_spectrum > confidence_interval_95_upper\n",
    "if significant_peaks.any(): \n",
    "    print(significant_peaks)\n",
    "else:\n",
    "    print(\"No significant peaks found.\")\n",
    "\n",
    "# The results show that there are four significant peaks in the power spectrum, but that is neglectable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Lat and Long for found and fallen meteorites\n",
    "lat_found = found['Lat']\n",
    "long_found = found['Long']\n",
    "lat_fallen = fallen['Lat']\n",
    "long_fallen = fallen['Long']\n",
    "lat_fallen_old = fallen_old['reclat']\n",
    "long_fallen_old = fallen_old['reclong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    # Calculate the great circle distance in kilometers between two points \n",
    "    # on the earth (specified in decimal degrees)\n",
    "\n",
    "    # Convert decimal degrees to radians \n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula \n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# reference point\n",
    "ref_latitude, ref_longitude = 0, 0\n",
    "\n",
    "distances = [haversine(ref_latitude, ref_longitude, lat, lon) for lat, lon in zip(lat_found, long_found)]\n",
    "\n",
    "# add ep to avoid division by zero\n",
    "# Calculate mean and standard deviation of 'distances'\n",
    "mean_distance = np.mean(distances)\n",
    "std_distance = np.std(distances)\n",
    "\n",
    "standardized_data = [(distance - mean_distance) / std_distance for distance in distances]\n",
    "\n",
    "# Generate a normal distribution with the same size as the meteorite data\n",
    "normal_distribution = np.random.normal(0, 1, size=len(standardized_data))\n",
    "\n",
    "# Perform the Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = stats.ks_2samp(standardized_data, normal_distribution)\n",
    "\n",
    "print(f'KS Statistic: {ks_statistic}, P-Value: {p_value}')\n",
    "\n",
    "x_values = np.linspace(min(standardized_data), max(standardized_data), 100)\n",
    "y_values = stats.norm.pdf(x_values, 0, 1)\n",
    "\n",
    "# plot a line of the normal distribution over the normalized data\n",
    "plt.plot(x_values, y_values, label='Normal Distribution')\n",
    "plt.hist(standardized_data, bins=50, density = True, label='Standardized Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lat  landratio\n",
    "\n",
    "\n",
    "0  -85.0   0.925334\n",
    "1  -75.0   0.655693\n",
    "2  -65.0   0.093650\n",
    "3  -55.0   0.009188\n",
    "4  -45.0   0.032247\n",
    "5  -35.0   0.112175\n",
    "6  -25.0   0.231537\n",
    "7  -15.0   0.220766\n",
    "8   -5.0   0.235444\n",
    "9    5.0   0.228707\n",
    "10  15.0   0.263686\n",
    "11  25.0   0.377645\n",
    "12  35.0   0.427399\n",
    "13  45.0   0.518811\n",
    "14  55.0   0.569828\n",
    "15  65.0   0.712646\n",
    "16  75.0   0.296525\n",
    "17  85.0   0.060094\n",
    "\n",
    "\n",
    "\n",
    "lon  landratio\n",
    "\n",
    "      \n",
    "0  -175.0   0.045740\n",
    "1  -165.0   0.061081\n",
    "2  -155.0   0.111033\n",
    "3  -145.0   0.128238\n",
    "4  -135.0   0.153409\n",
    "5  -125.0   0.234222\n",
    "6  -115.0   0.328145\n",
    "7  -105.0   0.371085\n",
    "8   -95.0   0.366712\n",
    "9   -85.0   0.339029\n",
    "10  -75.0   0.450235\n",
    "11  -65.0   0.508394\n",
    "12  -55.0   0.351325\n",
    "13  -45.0   0.297146\n",
    "14  -35.0   0.172011\n",
    "15  -25.0   0.147343\n",
    "16  -15.0   0.171506\n",
    "17   -5.0   0.330937\n",
    "18    5.0   0.357699\n",
    "19   15.0   0.542805\n",
    "20   25.0   0.637631\n",
    "21   35.0   0.564992\n",
    "22   45.0   0.487236\n",
    "23   55.0   0.395084\n",
    "24   65.0   0.377327\n",
    "25   75.0   0.429677\n",
    "26   85.0   0.435568\n",
    "27   95.0   0.468104\n",
    "28  105.0   0.514497\n",
    "29  115.0   0.495488\n",
    "30  125.0   0.447636\n",
    "31  135.0   0.416387\n",
    "32  145.0   0.368617\n",
    "33  155.0   0.209887\n",
    "34  165.0   0.137246\n",
    "35  175.0   0.089278"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine found\n",
    "# generate a dictionary using the lat as the key and the landratio as the value\n",
    "# They are in the nearest 10, for example, if it is -90 then it is the range from -90 to -80 \n",
    "# and 10 is from 0 to 10\n",
    "lat_land_ratio = {-90.0: 0.925334, -80.0: 0.655693, -70.0: 0.093650, -60.0: 0.009188, -50.0: 0.032247, \n",
    "                  -40.0: 0.112175, -30.0: 0.231537, -20.0: 0.220766, -10.0: 0.235444, 0.0: 0.23, 10.0: 0.228707, \n",
    "                  20.0: 0.263686, 30.0: 0.377645, 40.0: 0.427399, 50.0: 0.518811, 60.0: 0.569828, 70.0: 0.712646,\n",
    "                  80.0: 0.296525, 90.0: 0.060094}\n",
    "\n",
    "# Forming the theoretical cosine distribution according to the latitude data\n",
    "cosine_lat_theoretical = np.linspace(-np.pi/2, np.pi/2, len(lat_found))\n",
    "cosine_pdf = np.cos(cosine_lat_theoretical)\n",
    "\n",
    "# multiply the cosine_pdf by the land ratio for each latitude\n",
    "for i in range(len(cosine_lat_theoretical)):\n",
    "    latc = np.degrees(cosine_lat_theoretical[i])\n",
    "    # Find the range that the latitude falls into\n",
    "    lat_key = round(latc / 10) * 10\n",
    "    cosine_pdf[i] *= lat_land_ratio[lat_key]\n",
    "\n",
    "print(cosine_pdf)\n",
    "\n",
    "# Turning it to cdf to compare with our data\n",
    "cosine_pdf /= np.trapz(cosine_pdf, cosine_lat_theoretical) \n",
    "cosine_cdf = np.cumsum(cosine_pdf)\n",
    "cosine_cdf /= cosine_cdf[-1]  \n",
    "\n",
    "# rescaling our data to fit the distribution\n",
    "rescaled_found_lat = np.radians(lat_found)  # Convert degrees to radians\n",
    "ecdf_found = ECDF(rescaled_found_lat)\n",
    "\n",
    "ks_statistic, p_value = ks_2samp(ecdf_found(cosine_lat_theoretical), cosine_cdf)\n",
    "print(f\"KS Statistic: {ks_statistic}, P-Value: {p_value}\")\n",
    "\n",
    "# Plotting the KS graph\n",
    "plt.plot(cosine_lat_theoretical, cosine_cdf, label='Theoretical Cosine CDF')\n",
    "plt.text(1, 0.2, f'KS Statistic\\n{ks_statistic:.3f}', horizontalalignment='center')\n",
    "plt.step(ecdf_found.x, ecdf_found.y, label='Found CDF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the histogram of the (rescaled) latitude data and the \n",
    "# (not normalized) theoretical cosine distribution\n",
    "plt.hist(rescaled_found_lat, bins=50, density=True, alpha=0.6, label='Found Data')\n",
    "plt.plot(cosine_lat_theoretical, cosine_pdf, label='Cosine Distribution', color='red')\n",
    "plt.xlabel('Latitude (radians)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Comparison of Our Data with Cosine Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine fall\n",
    "cosine_fall_lat_theoretical = np.linspace(-np.pi/2, np.pi/2, len(lat_fallen_old))\n",
    "cosine_fall_pdf = np.cos(cosine_fall_lat_theoretical)\n",
    "\n",
    "# multiply the cosine_pdf by the land ratio for each latitude\n",
    "for i in range(len(cosine_fall_lat_theoretical)):\n",
    "    latc_fall = np.degrees(cosine_fall_lat_theoretical[i])\n",
    "    # Find the range that the latitude falls into\n",
    "    lat_key_fall = round(latc_fall / 10) * 10\n",
    "    cosine_fall_pdf[i] *= lat_land_ratio[lat_key_fall]\n",
    "\n",
    "\n",
    "# Turning it to cdf to compare with our data\n",
    "cosine_fall_pdf /= np.trapz(cosine_fall_pdf, cosine_fall_lat_theoretical) \n",
    "cosine_fall_cdf = np.cumsum(cosine_fall_pdf)\n",
    "cosine_fall_cdf /= cosine_fall_cdf[-1]\n",
    "\n",
    "rescaled_fall_lat = np.radians(lat_fallen_old)  # Convert degrees to radians\n",
    "ecdf_fall = ECDF(rescaled_fall_lat)\n",
    "\n",
    "ks_fall_statistic, fall_p_value = ks_2samp(ecdf_fall(cosine_fall_lat_theoretical), cosine_fall_cdf)\n",
    "print(f\"KS Statistic: {ks_fall_statistic}, P-Value: {fall_p_value}\")\n",
    "\n",
    "# Plotting the KS graph\n",
    "plt.plot(cosine_fall_lat_theoretical, cosine_fall_cdf, label='Theoretical Cosine CDF')\n",
    "plt.text(-1, 0.6, f'KS Statistic\\n{ks_fall_statistic:.3f}', horizontalalignment='center')\n",
    "plt.step(ecdf_fall.x, ecdf_fall.y, label='Fallen CDF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the histogram of the (rescaled) latitude data and the \n",
    "# (not normalized) theoretical cosine distribution\n",
    "plt.hist(rescaled_fall_lat, bins=50, density=True, alpha=0.6, label='Fallen Data')\n",
    "plt.plot(cosine_fall_lat_theoretical, cosine_fall_pdf, label='Cosine Distribution', color='red')\n",
    "plt.xlabel('Latitude (radians)')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Comparison of Our Data with Cosine Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform found\n",
    "# Generate a dictionary using the long as the key and the landratio as the value\n",
    "# They are in the nearest 10, for example, if it is -180 then it is the range from -180 to -170 \n",
    "# and 10 is from 0 to 10\n",
    "long_landratio = {-180.0: 0.045740, -170.0: 0.061081, -160.0: 0.111033, -150.0: 0.128238,\n",
    "                  -140.0: 0.153409, -130.0: 0.234222, -120.0: 0.328145, -110.0: 0.371085, \n",
    "                  -100.0: 0.366712, -90.0: 0.339029, -80.0: 0.450235, -70.0: 0.508394, \n",
    "                  -60.0: 0.351325, -50.0: 0.297146, -40.0: 0.172011, -30.0: 0.147343, -20.0: 0.171506,\n",
    "                  -10.0: 0.330937, 0.00: 0.34, 10.0: 0.357699, 20.0: 0.542805, 30.0: 0.637631, 40.0: 0.564992,\n",
    "                  50.0: 0.487236, 60.0: 0.395084, 70.0: 0.377327, 80.0: 0.429677, 90.0: 0.435568,\n",
    "                  100.0: 0.468104, 110.0: 0.514497, 120.0: 0.495488, 130.0: 0.447636, 140.0: 0.416387,\n",
    "                  150.0: 0.368617, 160.0: 0.209887, 170.0: 0.137246, 180.0: 0.089278}\n",
    "\n",
    "longitudes = np.array(long_found) \n",
    "\n",
    "# multiply the uniform distribution by the land ratio for each longitude\n",
    "# Define the range of your longitude data\n",
    "min_long = -180\n",
    "max_long = 180\n",
    "\n",
    "# Calculate the height of the uniform PDF (flat line)\n",
    "# Since it's a uniform distribution, the height is 1 divided by the range\n",
    "uniform_pdf_height = 1 / (max_long - min_long)\n",
    "\n",
    "# scale the uniform distribution by the water land ratio\n",
    "scaled_values = {longitude: ratio * uniform_pdf_height for longitude, ratio in long_landratio.items()}\n",
    "scaled_longitudes = []\n",
    "for longitude, scaled_ratio in scaled_values.items():\n",
    "    repetitions = int(scaled_ratio * 10000)\n",
    "    scaled_longitudes.extend([longitude] * repetitions)\n",
    "\n",
    "# Perform the KS test of the longitude against the uniform distribution that was multiplied to the land ratio\n",
    "found_statistic, found_p_value = ks_2samp(longitudes, scaled_longitudes)\n",
    "\n",
    "print(f\"KS statistic: {found_statistic}\")\n",
    "print(f\"P-value: {found_p_value}\")\n",
    "\n",
    "scaled_longitudes_for_plot = [scaled_values[round(long / 10) * 10] for long in np.arange(-180, 181, 10)]\n",
    "\n",
    "plt.hist(longitudes, bins=30, density=True, alpha=0.6, label='Found Data')\n",
    "plt.plot(np.arange(-180, 181, 10), scaled_longitudes_for_plot, color='r', label='Uniform PDF')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Our data vs. Uniform Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data for CDF plotting\n",
    "data_sorted = np.sort(longitudes)\n",
    "uniform_sorted = np.sort(scaled_longitudes)\n",
    "\n",
    "# Calculating the CDFs\n",
    "cdf_data = np.arange(len(data_sorted)) / len(data_sorted)\n",
    "cdf_uniform = np.arange(len(uniform_sorted)) / len(uniform_sorted)\n",
    "\n",
    "ks_y_values = np.interp(uniform_sorted, data_sorted, cdf_data)\n",
    "ks_distance = np.max(np.abs(cdf_uniform - ks_y_values))\n",
    "ks_x_values = uniform_sorted[np.abs(cdf_uniform - ks_y_values) == ks_distance]\n",
    "\n",
    "# Plotting the CDFs\n",
    "plt.plot(data_sorted, cdf_data, label='Found CDF')\n",
    "plt.plot(uniform_sorted, cdf_uniform, label='Uniform CDF')\n",
    "plt.text(ks_x_values[0], 0.5, f'KS Statistic\\n{found_statistic:.3f}', horizontalalignment='center')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF Comparison and KS Statistic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fallen meteorites\n",
    "\n",
    "longitudes_fall = np.array(long_fallen_old) \n",
    "\n",
    "# Perform the KS test of the longitude against the uniform distribution that was multiplied to the land ratio\n",
    "fall_long_statistic, fall_long_p_value = ks_2samp(longitudes_fall, scaled_longitudes)\n",
    "\n",
    "print(f\"KS statistic: {fall_long_statistic}\")\n",
    "print(f\"P-value: {fall_long_p_value}\")\n",
    "\n",
    "plt.hist(longitudes_fall, bins=30, density=True, alpha=0.6, label='Fallen Data')\n",
    "plt.plot(np.arange(-180, 181, 10), scaled_longitudes_for_plot, color='r', label='Uniform PDF')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('Our data vs. Uniform Distribution')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data for CDF plotting\n",
    "data_fall = np.sort(longitudes_fall)\n",
    "\n",
    "# Calculating the CDFs\n",
    "cdf_fall = np.arange(len(data_fall)) / len(data_fall)\n",
    "\n",
    "ks_y_fall = np.interp(uniform_sorted, data_fall, cdf_fall)\n",
    "ks_distance_fall = np.max(np.abs(cdf_uniform - ks_y_fall))\n",
    "ks_x_fall = uniform_sorted[np.abs(cdf_uniform - ks_y_fall) == ks_distance_fall]\n",
    "\n",
    "# Plotting the CDFs\n",
    "plt.plot(data_fall, cdf_fall, label='Fallen CDF')\n",
    "plt.plot(uniform_sorted, cdf_uniform, label='Uniform CDF')\n",
    "plt.text(ks_x_fall[0], 0.5, f'KS Statistic\\n{fall_long_statistic:.3f}', horizontalalignment='center')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF Comparison and KS Statistic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
